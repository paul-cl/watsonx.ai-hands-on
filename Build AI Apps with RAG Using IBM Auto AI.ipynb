{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b855cc",
   "metadata": {},
   "source": [
    "# IBM Auto AI RAG 사용하기\n",
    "IBM Auto AI RAG를 사용하여 RAG를 만들고 배포한 AI 서비스를 LangChain과 결합하여 최종 응답을 처리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12e5d8",
   "metadata": {},
   "source": [
    "# 라이브러리 설치하기\n",
    "Lab에서 Auto AI RAG를 사용하기 위해서는 아래의 라이브러리를 설치해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!pip install langchain==0.3.0\n",
    "!pip install langchain --upgrade\n",
    "\n",
    "!pip install langchain_openai\n",
    "!pip install langchain_community\n",
    "\n",
    "!pip install -U langchain-ibm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf8aa8",
   "metadata": {},
   "source": [
    "라이브러리 설치가 완료 되었으면 노트북 커널을 Restart 합니다.\n",
    "메뉴에서 `kernel > restart` 를 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a699ad3",
   "metadata": {},
   "source": [
    "# IBM Auto RAG AI 서비스를 적용한 RAG 시스템 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Document 객체로 변환하는 함수\n",
    "def convert_to_documents(reference_docs):\n",
    "    documents = []\n",
    "    for doc in reference_docs:\n",
    "        page_content = doc.get(\"page_content\", \"\")\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        \n",
    "        # LangChain Document 객체 생성\n",
    "        document = Document(\n",
    "            page_content=page_content,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d2bde",
   "metadata": {},
   "source": [
    "\"CLOUD_API_KEY\"는 IBM Cloud의 watsonx 서비스에서 API KEY 정보를 가져옵니다.   \n",
    "\"MY_PROJECT_ID\"는 IBM Cloud의 watsonx 서비스에서 프로젝트 ID 정보를 가져옵니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd37b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM Cloud API Key와 URL 설정\n",
    "api_key = \"<CLOUD_API_KEY>\"\n",
    "# region에 따라 주소가 다를 수 있습니다. 주소를 확인해 주세요.\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\" \n",
    "project_id = \"<MY_PROJECT_ID>\"\n",
    "\n",
    "# 배포한 AutoAI RAG 모델의 URL\n",
    "auto_ai_rag_url = \"<IBM_AUTOAI_RAG_URL>\"\n",
    "auto_ai_rag_stream_url = \"<IBM_AUTOAI_RAG_STREAM_URL>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ddfbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '기업의 생성형AI 도입 현황은?'\n",
    "# user_input = \"기업에서 생성형 AI를 도입하는 가치는?\"\n",
    "# user_input = \"기업에서 생성형 AI를 도입하기 위해 고려사항은?\"\n",
    "# user_input = \"기업에서 생성형 AI 도입을 가로막는 가장 큰 장애물은?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = api_key\n",
    "token_response = requests.post(\n",
    "    'https://iam.cloud.ibm.com/identity/token',\n",
    "    data={\n",
    "        \"apikey\": API_KEY,\n",
    "        \"grant_type\": 'urn:ibm:params:oauth:grant-type:apikey'\n",
    "    }\n",
    ")\n",
    "mltoken = token_response.json().get(\"access_token\")\n",
    "if not mltoken:\n",
    "    raise Exception(\"토큰 발급 실패!\")\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {mltoken}',\n",
    "    'Accept': 'text/event-stream'  # 스트리밍 응답 처리용 헤더\n",
    "}\n",
    "\n",
    "payload_scoring = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_input\n",
    "        }\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    auto_ai_rag_stream_url,\n",
    "    headers=headers,\n",
    "    json=payload_scoring,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"\\n  AI 응답:\")\n",
    "full_text = \"\"\n",
    "reference_docs = []\n",
    "\n",
    "try:\n",
    "    for line in response.iter_lines(decode_unicode=True):\n",
    "        if line.startswith(\"data: \"):\n",
    "            json_str = line[6:]\n",
    "            if json_str.strip() == \"[DONE]\":\n",
    "                break\n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "\n",
    "                # content 추가\n",
    "                delta = data.get(\"choices\", [{}])[0].get(\"delta\", {})\n",
    "                content_piece = delta.get(\"content\", \"\")\n",
    "                if content_piece:\n",
    "                    print(content_piece, end=\"\", flush=True)\n",
    "                    full_text += content_piece\n",
    "\n",
    "                # reference_documents 저장\n",
    "                if \"reference_documents\" in data.get(\"choices\", [{}])[0]:\n",
    "                    reference_docs = data[\"choices\"][0][\"reference_documents\"]\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"\\n JSON 파싱 오류: {json_str}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n 스트리밍 중 예외 발생: {e}\")\n",
    "\n",
    "print(\"\\n  인용된 문서 내용:\")\n",
    "for idx, doc in enumerate(reference_docs):\n",
    "    try:\n",
    "        page_content = doc.get(\"page_content\", \"\")\n",
    "        print(f\"\\n[문서 {idx + 1}]\\n{page_content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"문서 {idx + 1} 출력 실패: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00704f",
   "metadata": {},
   "source": [
    "# Rangchain 을 사용한 RAG 시스템 구현\n",
    "Langchain은 다양한 데이터 소스와 AI 모델을 연결하여 RAG 시스템을 구축하는 데 유용한 라이브러리입니다. Langchain을 사용하면 데이터 소스에서 정보를 검색하고, AI 모델을 사용하여 새로운 텍스트를 생성하는 작업을 쉽게 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d183c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def call_ai_service (user_input):\n",
    "\n",
    "    API_KEY = api_key\n",
    "    token_response = requests.post(\n",
    "        'https://iam.cloud.ibm.com/identity/token',\n",
    "        data={\n",
    "            \"apikey\": API_KEY,\n",
    "            \"grant_type\": 'urn:ibm:params:oauth:grant-type:apikey'\n",
    "        }\n",
    "    )\n",
    "    mltoken = token_response.json().get(\"access_token\")\n",
    "    if not mltoken:\n",
    "        raise Exception(\"토큰 발급 실패!\")\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {mltoken}',\n",
    "        'Accept': 'text/event-stream'  # 스트리밍 응답 처리용 헤더\n",
    "    }\n",
    "\n",
    "    payload_scoring = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        auto_ai_rag_stream_url,\n",
    "        headers=headers,\n",
    "        json=payload_scoring,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # print(\"\\n  AI 응답:\")\n",
    "    full_text = \"\"\n",
    "    reference_docs = []\n",
    "\n",
    "    try:\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if line.startswith(\"data: \"):\n",
    "                json_str = line[6:]\n",
    "                if json_str.strip() == \"[DONE]\":\n",
    "                    break\n",
    "                try:\n",
    "                    data = json.loads(json_str)\n",
    "\n",
    "                    # content 추가\n",
    "                    delta = data.get(\"choices\", [{}])[0].get(\"delta\", {})\n",
    "                    content_piece = delta.get(\"content\", \"\")\n",
    "                    if content_piece:\n",
    "                        # print(content_piece, end=\"\", flush=True)\n",
    "                        full_text += content_piece\n",
    "\n",
    "                    # reference_documents 저장\n",
    "                    if \"reference_documents\" in data.get(\"choices\", [{}])[0]:\n",
    "                        reference_docs = data[\"choices\"][0][\"reference_documents\"]\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"\\n JSON 파싱 오류: {json_str}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n 스트리밍 중 예외 발생: {e}\")\n",
    "\n",
    "    # print(\"\\n  인용된 문서 내용:\")\n",
    "    for idx, doc in enumerate(reference_docs):\n",
    "        try:\n",
    "            page_content = doc.get(\"page_content\", \"\")\n",
    "            # print(f\"\\n[문서 {idx + 1}]\\n{page_content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"문서 {idx + 1} 출력 실패: {e}\")\n",
    "    return reference_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c39e44",
   "metadata": {},
   "source": [
    "watsionx.ai는 IBM의 AI 플랫폼으로, 다양한 AI 모델과 서비스를 제공합니다. watsonx.ai의 AI 모델에 연결하고, 데이터를 검색하고, 새로운 텍스트를 생성하는 작업을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"WATSONX_URL\"] = ibm_cloud_url\n",
    "os.environ[\"WATSONX_APIKEY\"] = api_key\n",
    "# os.environ[\"WATSONX_TOKEN\"] = \"\"\n",
    "\n",
    "params = {\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"min_new_tokens\": 50,\n",
    "    \"max_new_tokens\": 1000\n",
    "}\n",
    "\n",
    "# WatsonxLLM 모델 초기화\n",
    "model_llm = WatsonxLLM(\n",
    "    model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "    project_id=project_id,\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a0d06",
   "metadata": {},
   "source": [
    "배포한 Auto RAG AI 서비스에서 데이터를 검색 후 Langchain과 연동하기 위해 데이타 타입을 Document 형식으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80491569",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '기업의 생성형AI 도입 현황은?'\n",
    "# user_input = \"기업에서 생성형 AI를 도입하는 가치는?\"\n",
    "# user_input = \"기업에서 생성형 AI를 도입하기 위해 고려사항은?\"\n",
    "# user_input = \"기업에서 생성형 AI 도입을 가로막는 가장 큰 장애물은?\"\n",
    "\n",
    "\n",
    "reference_docs = call_ai_service(user_input)\n",
    "# 변환된 문서 리스트\n",
    "docs_search = convert_to_documents(reference_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d231f3",
   "metadata": {},
   "source": [
    "watsonx에 배포한 AI 서비스의 응답 결과를 Langchain과 결합하여 RAG 시스템을 구축할 수 있습니다. Langchain은 다양한 데이터 소스와 AI 모델을 연결하여 RAG 시스템을 구축하는 데 유용한 라이브러리입니다. Langchain을 사용하면 데이터 소스에서 정보를 검색하고, AI 모델을 사용하여 새로운 텍스트를 생성하는 작업을 쉽게 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 정의\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "아래 문서를 참고해서 사용자 질문에 정확하게 한국어로 답변해줘.\n",
    "\n",
    "문서:\n",
    "{context}\n",
    "\n",
    "질문: {input}\n",
    "\n",
    "답변:\n",
    "\"\"\")\n",
    "\n",
    "# 문서 체인 구성\n",
    "stuff_chain = create_stuff_documents_chain(model_llm, prompt)\n",
    "\n",
    "\n",
    "# 실행\n",
    "result = stuff_chain.invoke({\"context\": docs_search, \"input\":user_input})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
