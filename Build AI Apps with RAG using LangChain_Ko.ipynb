{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64d075d",
   "metadata": {},
   "source": [
    "# Hands-on: Build AI Apps with RAG using watsonx.ai, LangChain & Vector Database\n",
    "\n",
    "## Overview\n",
    "\n",
    "이 실습에서는 LLM 애플리케이션 구축을 위한 프레임워크인 LangChain을 사용하게 됩니다.\n",
    "다음 내용을 학습합니다.:\n",
    "1. Simple Prompt to LLM using LangChain\n",
    "2. Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n",
    "3. Sequential Prompts using Simple Sequential Chain\n",
    "4. Retrieval Question Answering (RAG)\n",
    "5. Documents Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c33201",
   "metadata": {},
   "source": [
    "## 1. Simple Prompt to LLM (수동으로 프롬프트 템플릿 생성)\n",
    "- watsonx에서 LLM에 프롬프트를 보내는 기본 사용 사례(Langchain을 사용하지 않음)\n",
    "- 이 예에서는 LLM 모델(Google flan-ul2)에 직접 간단한 프롬프트를 보냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226425e",
   "metadata": {},
   "source": [
    "### 종속 항목 설치 및 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664a584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install library\n",
    "!pip install chromadb==0.4.2\n",
    "!pip install langchain==0.0.312\n",
    "!pip install langchain --upgrade\n",
    "!pip install flask-sqlalchemy --user\n",
    "!pip install pypdf \n",
    "!pip install sentence-transformers\n",
    "!pip install langchain_openai\n",
    "!pip install -U langchain-ibm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-ibm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adcdb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from langchain import PromptTemplate # Langchain Prompt Template\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain # Langchain Chains\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\n",
    "from langchain.text_splitter import CharacterTextSplitter # Text splitter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81111c9",
   "metadata": {},
   "source": [
    "### WML 인증정보 설정하기\n",
    "이 셀은 watsonx Foundation Model 추론 작업에 필요한 WML 자격 증명을 위해 IBM Cloud API 키를 사용합니다.\n",
    "\n",
    "**Action:** IBM Cloud의 사용자 API 키를 생성합니다. 자세한 내용은 다음을 참조하세요.\n",
    "[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65554e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key and URL from .env\n",
    "#load_dotenv()\n",
    "api_key = \"<YOUR API KEY HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c24f1",
   "metadata": {},
   "source": [
    "### watsonx.ai 플랫폼의 project id 정보 설정하기\n",
    "Foundation Model 호출을 위해서는 watsonx.ai 플랫폼에서 생성한 프로젝트의 ID가 필요합니다.  \n",
    "Cloud region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96339420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# region에 따라 주소가 다를 수 있습니다. 주소를 확인해 주세요.\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\" \n",
    "project_id = \"<YOUR PROJECT ID HERE>\"\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# watsonx model 초기화\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 25,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 20\n",
    "}\n",
    "\n",
    "llm_model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "print(\"Done initializing LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple prompt를 모델에 전송하기\n",
    "countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "\n",
    "try:\n",
    "  for country in countries:\n",
    "    question = f\"What is the capital of {country}\"\n",
    "    res = llm_model.generate_text(question)\n",
    "    print(f\"The capital of {country} is {res.capitalize()}\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf74712",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Prompt and Few-Shot Prompt using LangChain's Prompt Template\n",
    "- 실제 사용 사례는 더 복잡할 수 있습니다. LLM에 일반 프롬프트를 보내는 대신 Langchain 프롬프트 템플릿을 사용하고 있습니다.\n",
    "- 이 예에서는 Langchain 프롬프트 템플릿을 사용하여 LLM 모델(Google flan-ul2)에 프롬프트를 보냅니다.\n",
    "- 프롬프트 템플릿 사용의 장점:\n",
    "    1. **모듈성**: 프롬프트 템플릿을 사용하면 구조화된 템플릿을 한 번 정의하고 이를 다른 입력 변수와 함께 재사용할 수 있습니다. 이렇게 하면 코드가 더 모듈화되고 유지 관리가 더 쉬워집니다.\n",
    "     2. **동적 입력**: 프롬프트 템플릿에서는 이 예의 \"국가\"와 같은 동적 입력 변수를 허용합니다. 이는 전체 프롬프트 구조를 수정하지 않고도 입력 값을 쉽게 변경할 수 있음을 의미합니다.\n",
    "     3. **가독성**: 템플릿은 프롬프트에 명확한 구조를 제공하므로 다른 개발자가 프롬프트의 목적과 프롬프트가 모델과 상호 작용하는 방식을 더 쉽게 이해할 수 있습니다.\n",
    "     4. **유연성**: 특정 사용 사례나 도메인 요구 사항에 맞게 템플릿을 사용자 정의할 수 있습니다. 이러한 유연성을 통해 전체 프롬프트 논리를 다시 작성하지 않고도 프롬프트를 다양한 시나리오에 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92d3c2",
   "metadata": {},
   "source": [
    "### 2.1 Zero-shot Prompt\n",
    "- Zero-shot Prompt는 가장 간단한 유형의 프롬프트입니다. 모델에 대한 예제는 제공하지 않고 지침만 제공합니다.\n",
    "- 지시 사항을 질문으로 표현할 수 있습니다. 예: *\"제너레이티브 AI의 개념을 설명하세요.\"*\n",
    "- 모델에게 '역할'을 부여할 수도 있습니다. 예: *\"당신은 데이터 과학자입니다. 생성 AI의 개념을 설명하십시오.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b140fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prompt template 정의\n",
    "prompt = PromptTemplate(\n",
    "  input_variables = [\"country\"],\n",
    "  template = \"What is the capital of {country}?\",\n",
    ")\n",
    "\n",
    "try:\n",
    "  # Langchain을 사용하려면 Langchain 확장을 인스턴스화해야 합니다.\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # 생성한 모델과 프롬프트를 기반으로 하는 체인 정의\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a5340",
   "metadata": {},
   "source": [
    "### 2.2 Few-shot Prompt\n",
    "- Few-shot Prompt는 향후 유사한 작업을 처리하는 방법을 파악하기 위해 모델에 몇 가지 예를 제공합니다.\n",
    "- 모델이 작업을 더 잘 이해하는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572caefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Few -shot 예제를 입력합니다.\n",
    "examples = [\n",
    "    {\"input\": \"What is the capital of Sweden?\", \"output\": \"Stockholm\"},\n",
    "    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"Kuala Lumpur\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #('system', 'You are a helpful AI Assistant'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57575ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Langchain을 사용하려면 Langchain 확장을 인스턴스화해야 합니다.\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n",
    "\n",
    "  # LLM에 추론합니다.\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    prompt = f\"What is the capital of {country}?\"\n",
    "    print(prompt)\n",
    "    response = chain.run(prompt)\n",
    "    print(response)\n",
    "    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Few -shot 예제를 입력합니다.\n",
    "examples = [\n",
    "    {\"input\": \"What is the capital of Sweden?\", \"output\": \"The capital of Sweden is Stockholm\"},\n",
    "    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"The capital of Malaysia is Kuala Lumpur\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #('system', 'You are a helpful AI Assistant'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Langchain을 사용하려면 Langchain 확장을 인스턴스화해야 합니다.\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # 생성한 모델과 프롬프트를 기반으로 하는 체인 정의\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    prompt = f\"What is the capital of {country}?\"\n",
    "    print(prompt)\n",
    "    response = chain.run(prompt)\n",
    "    print(response)\n",
    "    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4674b8",
   "metadata": {},
   "source": [
    "## 3. Sequential Prompts using Simple Sequential Chain\n",
    "- LangChain의 Simple Sequential Chain을 사용하면 여러 프롬프트를 쉽게 연결하여 순차적 프롬프트를 만들 수 있습니다.\n",
    "- 순차 프롬프트라고도 하는 프롬프트 체인을 사용하면 하나의 프롬프트에 대한 응답이 시퀀스의 다음 프롬프트에 대한 입력이 될 수 있습니다.\n",
    "- 각각의 후속 프롬프트는 AI의 이전 응답을 통해 정보를 받아 모델의 출력을 점진적으로 개선하는 일련의 상호 작용을 생성합니다.\n",
    "- 참조사이트: [SimpleSequentialChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e999911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개의 sequential prompts 생성\n",
    "\n",
    "# 주어진 토픽에 대해 랜덤한 질문을 생성하는 프롬프트 템플릿\n",
    "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
    "\n",
    "# 주어진 질문에 답변하는 프롬프트 템플릿\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4aabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 모델 인스턴스화(참고, 사용 사례에 따라 모델이 다를 수 있음)\n",
    "# 위와 같이 WatsonxLLM 래퍼를 반환하는 .to_langchain() 메서드에 주목하세요.\n",
    "model_1 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "model_2 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential chain 실행\n",
    "prompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\n",
    "prompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6495db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \"animal\"이라는 주제로 체인을 운영합니다.\n",
    "# 출력을 확인하기 위해 다양한 주제를 제공하면서 놀아보세요. 예. car, the Roman empire\n",
    "try:\n",
    "  qa.run(\"an animal\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd84ba9",
   "metadata": {},
   "source": [
    "## 4. Retrieval Question Answering (RAG)\n",
    "- LangChain의 검색 질문 응답(QA)을 사용하면 문서에서 프롬프트(질문)에 대한 답변으로 쉽게 구절을 추출할 수 있습니다.\n",
    "\n",
    "\n",
    "### 영어로 된 문서에 질의하기\n",
    "- 먼저, 이 링크에서 샘플 PDF 파일을 다운로드하세요.: [what_is_generative_ai.pdf](https://ibm.box.com/v/what-is-generative-ai)\n",
    "- 그런 다음 파일을 프로젝트에 업로드하고 액세스 토큰을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create access token in project\n",
    "token = \"<YOUR ACCESS TOKEN HERE>\"\n",
    "wslib = access_project_or_space({\"token\":token})\n",
    "wslib.download_file(\"what_is_generative_ai.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d0893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load PDF document\n",
    "pdf = 'what_is_generative_ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb2f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Index loaded PDF\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding = HuggingFaceEmbeddings(),\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e723ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# watsonx google/flan-ul2 model 초기화\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.MAX_NEW_TOKENS: 300\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ae488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain 초기화\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4891cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 기반 질의\n",
    "res = chain.run(\"What is Machine Learning?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20564da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 기반 질의\n",
    "res = chain.run(\"What are the problems generative AI can solve?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 기반 질의\n",
    "res = chain.run(\"What are the risks of Generative AI?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a95886",
   "metadata": {},
   "source": [
    "### 한글 문서에 질의 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c690082",
   "metadata": {},
   "outputs": [],
   "source": [
    "wslib = access_project_or_space({\"token\":token})\n",
    "wslib.download_file(\"ibk_card.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF document 로딩\n",
    "pdf = 'ibk_card.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distiluse-base-multilingual-cased-v1\"\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "        embedding=HuggingFaceEmbeddings(model_name=model_name),\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)).from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28378bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.MAX_NEW_TOKENS: 3000\n",
    "}\n",
    "\n",
    "# 한글 지원 모델로 변경\n",
    "model = Model(\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain 초기화\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71052171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 기반 질문하기\n",
    "res = chain.run(\"카드사가 부가서비스를 변경할 수 있어?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 기반 질문하기\n",
    "res = chain.run(\"카드사가 부가서비스를 변경할 경우 고지방법은?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 기반 질문하기\n",
    "res = chain.run(\"일시불 거래 연체 시 어떻게 되?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187e120",
   "metadata": {},
   "source": [
    "## 5. Documents Summarization\n",
    "- 텍스트 요약은 긴 텍스트에 대해 짧지만 유익한 요약을 만드는 NLP의 작업입니다. LLM은 뉴스 기사, 연구 논문, 기술 문서 및 기타 종류의 텍스트를 요약하는 데 사용할 수 있습니다.\n",
    "- 긴 문서를 요약하는 것은 어려울 수 있습니다. 요약을 생성하려면 색인된 문서에 요약 전략을 적용해야 합니다.\n",
    "- 이 예에서는 다음 3개 웹사이트의 긴 문서를 요약합니다.\n",
    "     - https://www.gttkorea.com/news/articleView.html?idxno=9138\n",
    "     - https://www.hankyung.com/article/202405136945Y\n",
    "     - https://www.apple-economy.com/news/articleView.html?idxno=73093\n",
    "- 요약기 앱을 구축할 때 문서를 LLM의 컨텍스트 창으로 전달하는 방법은 다음과 같습니다.\n",
    "     1. **방법 1: Stuff** - \"Stuff\"는 전체 문서 내용을 단일 프롬프트에 포함하면 됩니다. (가장 간단한 방법)\n",
    "     2. **방법 2: MapReduce** - \"Map\" 단계에서 각 문서를 자체적으로 요약한 다음, \"Reduce\" 단계에서 최종 요약 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551b02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install library\n",
    "!pip3 install transformers chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8903f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaaddb9",
   "metadata": {},
   "source": [
    "### 5.1 Method 1: Stuff\n",
    "- 단순히 모든 문서를 단일 프롬프트로 “넣는” 방식입니다. 이는 가장 간단한 접근 방식입니다.\n",
    "- 체인의 `chain_type` 모드를 `stuff`설정 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fcefa",
   "metadata": {},
   "source": [
    "### Stuff without using Prompt Template\n",
    "- 프롬프트 및 LLM 파이프라인은 `load_summarize_chain`이라는 단일 개체로 래핑됩니다.\n",
    "- `stuff`를 `chain_type`으로 설정합니다.\n",
    "- 이 예에서는 비교적 짧은 문서가 성공적으로 요약되는 것을 볼 수 있습니다.\n",
    "- 문서 : [AI 기술 발전에 따라 ‘AI 거버넌스’ 요구도 급증세](https://www.gttkorea.com/news/articleView.html?idxno=9138)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38710b4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba2d0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# document loader 초기화\n",
    "loader = WebBaseLoader(\"https://www.gttkorea.com/news/articleView.html?idxno=9138\")\n",
    "doc = loader.load()\n",
    "\n",
    "# watsonx google/flan-t5-xxl 모델 초기화\n",
    "# 결과를 최적화하기 위해 일부 런타임 매개변수를 조정해야 할 수도 있습니다.\n",
    "\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"greedy\",\n",
    "    GenParams.REPETITION_PENALTY: 1.1,\n",
    "    GenParams.MIN_NEW_TOKENS: 20,\n",
    "    GenParams.MAX_NEW_TOKENS: 1024\n",
    "}\n",
    "\n",
    "llm_model = Model(\n",
    "    model_id=\"meta-llama/llama-3-70b-instruct\", \n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "# chain_type을 'stuff' 설정\n",
    "chain = load_summarize_chain(llm_model, chain_type=\"stuff\")\n",
    "\n",
    "#summarization task 실행\n",
    "res = chain.run(doc)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb53a81",
   "metadata": {},
   "source": [
    "### Stuff using Prompt Template\n",
    "- 문서를 프롬프트 템플릿에 로드하고 \"stuffed document chain\"을 실행합니다. 문서 목록도 채울 수 있습니다.\n",
    "- `StuffDocumentsChain`은 `load_summarize_chain` 메소드의 일부로 사용됩니다.\n",
    "- 이 예에서는 위와 동일한 요약 출력이 표시됩니다.\n",
    "- 참조사이트: [StuffDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c8e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import librararies\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# prompt 정의\n",
    "# <|user|> 는 랭체인 마크업언어로 사용자 입력을 나타내는 마크입니다.\n",
    "# <|assistant|>는 모델 출력을 나태내는 마크 입니다.\n",
    "prompt_template = \"\"\"\n",
    "<|user|>\n",
    "다음에 주어진 문서에 대해서 간결한 요약을 작성하시오. 반드시 한국어로 답변하시오.\n",
    "일본어 또는 중국어 한자어가 포함되어 있으면 한국어로 말하시오.\n",
    "예) 需求 -> 필요\n",
    "\n",
    "문서: \"{text}\"\n",
    "\n",
    "<|assistant|>\n",
    "한국어 요약 결과:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# LLMs chain 정의\n",
    "llm_chain = LLMChain(llm=llm_model, prompt=prompt)\n",
    "\n",
    "# StuffDocumentsChain 정의\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "# summarization task 실행\n",
    "res = stuff_chain.run(doc)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938eb51",
   "metadata": {},
   "source": [
    "### LLM 토큰 제한으로 인한 'Stuff' 방법의 제한\n",
    "- 이 예에서는 더 많은 문서를 추가하면(토큰이 늘어남) 다음 오류가 발생하는 것을 볼 수 있습니다. `입력 토큰 수 8649는 이 모델에 대한 총 토큰 제한 8192를 초과할 수 없습니다.`\n",
    "- 이는 모델의 토큰 제한(최대 컨텍스트 창 길이) 때문입니다.\n",
    "- LangChain을 사용하면 청크 및 재귀적 요약 방법을 실행하는 `MapReduce`를 사용하여 이 문제를 해결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a new document from URL\n",
    "loader_2 = WebBaseLoader('https://www.hankyung.com/article/202405136945Y')\n",
    "doc_2 = loader_2.load()\n",
    "\n",
    "# Combine the new document to the previous document\n",
    "docs = doc + doc_2\n",
    "\n",
    "# Run the stuff chain\n",
    "try:\n",
    "  res = stuff_chain.run(docs)\n",
    "  print(res)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3f5e1",
   "metadata": {},
   "source": [
    "### 5.2 Method 2: MapReduce\n",
    "- 이 방법은 각 문서를 `map` 단계에서 개별적으로 요약한 다음, `reduce` 단계에서 요약본들을 최종 요약본으로 합치는 방식입니다.\n",
    "- Reference: [ReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain)\n",
    "- Reference: [MapReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from time import perf_counter\n",
    "\n",
    "# Add a 3rd document\n",
    "print(\"Loading 3rd document...\")\n",
    "loader_3 = WebBaseLoader(\"https://www.apple-economy.com/news/articleView.html?idxno=73093\")\n",
    "doc_3 = loader_3.load()\n",
    "docs = docs + doc_3\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"\n",
    "<|user|>\n",
    "여러개의 문서가 다음과 같이 주어집니다.\n",
    "{docs}\n",
    "해당 문서들의 주요 논점을 판단하세요.\n",
    "\n",
    "<|assistant|>\n",
    "한국어로 된 요약문을 생성하세요.\n",
    "한국어 답변 생성: 이 문서는 \"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "print(\"Init map chain...\")\n",
    "map_chain = LLMChain(llm=llm_model, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"\n",
    "<|user|>\n",
    "다음에 주어지는 내용은 요약된 내용입니다.\n",
    "{doc_summaries}\n",
    "해당 내용을 활용하여 최종으로 통합된 주요 주제를 요약하세요.\n",
    "\n",
    "<|assistant|>\n",
    "한국어로 된 최종 요약문을 생성하세요.\n",
    "한국어 답변 생성: 이 문서는 \"\"\"\n",
    "\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "print(\"Init reduce chain...\")\n",
    "reduce_chain = LLMChain(llm=llm_model, prompt=reduce_prompt)\n",
    "\n",
    "# 문서 목록을 가져와 단일 문자열로 결합한 다음 이를 LLMChain에 전달합니다.\n",
    "print(\"Stuff documents using reduce chain...\")\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# 매핑된 문서를 결합하고 반복적으로 축소합니다.\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # 이것이 호출되는 최종 체인입니다.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # 문서가 'StuffDocumentsChain'의 컨텍스트를 초과하는 경우\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # 문서를 그룹화할 최대 토큰 수입니다.\n",
    "    token_max=4000\n",
    ")\n",
    "\n",
    "# 체인을 매핑하여 문서를 결합한 다음 결과를 결합합니다.\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # 문서를 넣을 llm_chain의 변수 이름\n",
    "    document_variable_name=\"docs\",\n",
    "    # 출력에 맵 단계의 결과를 반환합니다.\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 여기서는 특히 flan-ul2 모델을 위해 Huggingface의 사전 훈련된 토크나이저를 사용하고 있습니다.\n",
    "# 결과가 어떻게 변하는지 보기 위해 다양한 토크나이저와 텍스트 분할기를 사용해 볼 수도 있습니다.\n",
    "print(\"Init chunk splitter...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-base\") # Hugging face tokenizer for flan-ul2\n",
    "    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"Using {len(split_docs)} chunks: \")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Run map-reduce chain. This should take ~15-30 seconds...\")\n",
    "try:\n",
    "    t1_start = perf_counter()\n",
    "    results = map_reduce_chain(split_docs)\n",
    "    steps = results[\"intermediate_steps\"]\n",
    "    output = results[\"output_text\"]\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n",
    "\n",
    "    print(\"Results from each chunk: \\n\")\n",
    "    for idx, step in enumerate(steps):\n",
    "        print(f\"{idx + 1}. {step}\\n\")\n",
    "    \n",
    "    print(\"\\n\\nFinal output:\\n\")\n",
    "    print(output)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c73c2",
   "metadata": {},
   "source": [
    "- 보시다시피 `Langchain`은 모델용 토크나이저와 함께 더 많은 양의 텍스트를 신속하게 덩어리로 나누고 간결한 문장 한두 개로 `반복적`으로 요약할 수 있습니다. 다양한 문서를 시도하고, 모델 런타임 매개변수를 조정하고, 다른 모델을 모두 시도하여 상황이 어떻게 작동하는지 확인해 볼 수 있습니다. 좋은 결과를 얻기 위해 주목해야 할 가장 중요한 사항 중 하나는 입력이 `청크화`되고 `토큰화`되는 방식이 매우 중요하다는 것입니다. 좋지 않은 `map` 결과를 전달하면 요약 품질이 낮아집니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
